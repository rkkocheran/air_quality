{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5732fdd",
   "metadata": {},
   "source": [
    "## A Comparative Study of Regional Air Quality\n",
    "\n",
    "This notebook uses webscraping to collect general city data for a comparative study of air quality between five cities with three significantly different terrains and two levels of population density:\n",
    "1. San Diego, California\n",
    "2. Los Angeles, California \n",
    "3. Denver, Colorado\n",
    "4. Atlanta, Georgia\n",
    "5. Nashville, Tennessee\n",
    "\n",
    "The data gathered in this notebook is available at: https://en.wikipedia.org/wiki/. The resulting data is saved as a DataFrame and exported as a CSV file titled _'city_data.csv'_.\n",
    "\n",
    "NOTES:<br>\n",
    "- The 'metro' population value is used in this analysis. This value is defined as 'A region that consists of a densely populated urban agglomeration and its surrounding territories sharing industries, commercial areas, transport network, infrastructures and housing. Metropolitan areas typically include satellite cities, towns and intervening rural areas that are socioeconomically tied to the principal cities or urban core, often measured by commuting patterns.<br>\n",
    ">(_Source: https://en.wikipedia.org/wiki/Metropolitan_area_)\n",
    "- The city's population rank is based on wikipedia's \"List of United States cities by population\".<br>\n",
    ">(_Source: https://en.wikipedia.org//wiki/List_of_United_States_cities_by_population_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0328f3",
   "metadata": {},
   "source": [
    "### Import the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31103a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b070c1c",
   "metadata": {},
   "source": [
    "### Declare the Static Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f8df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The url ids for each city\n",
    "city_id = {'San Diego':'San_Diego',\n",
    "           'Los Angeles':'Los_Angeles',\n",
    "           'Denver':'Denver',\n",
    "           'Atlanta':'Atlanta',\n",
    "           'Nashville':'Nashville'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0584520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The common url\n",
    "url = 'https://en.wikipedia.org/wiki/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5354c24",
   "metadata": {},
   "source": [
    "### Loop through Each City's Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9572b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>city_area_sqmi</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>pop_year</th>\n",
       "      <th>pop_density_sqmi</th>\n",
       "      <th>pop_metro</th>\n",
       "      <th>pop_rank</th>\n",
       "      <th>climate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>San Diego</td>\n",
       "      <td>California</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>330</td>\n",
       "      <td>60</td>\n",
       "      <td>2020</td>\n",
       "      <td>4260</td>\n",
       "      <td>3299000</td>\n",
       "      <td>17</td>\n",
       "      <td>Semi-Arid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>470</td>\n",
       "      <td>300</td>\n",
       "      <td>2020</td>\n",
       "      <td>8300</td>\n",
       "      <td>13201000</td>\n",
       "      <td>2</td>\n",
       "      <td>Semi-Arid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denver</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>Denver</td>\n",
       "      <td>150</td>\n",
       "      <td>5410</td>\n",
       "      <td>2020</td>\n",
       "      <td>4670</td>\n",
       "      <td>2964000</td>\n",
       "      <td>19</td>\n",
       "      <td>Semi-Arid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Fulton, DeKalb</td>\n",
       "      <td>140</td>\n",
       "      <td>890</td>\n",
       "      <td>2020</td>\n",
       "      <td>3690</td>\n",
       "      <td>6144000</td>\n",
       "      <td>8</td>\n",
       "      <td>Humid Subtropical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nashville</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>Davidson</td>\n",
       "      <td>500</td>\n",
       "      <td>600</td>\n",
       "      <td>2020</td>\n",
       "      <td>1420</td>\n",
       "      <td>1990000</td>\n",
       "      <td>36</td>\n",
       "      <td>Humid Subtropical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          city       state          county  city_area_sqmi  elevation_ft  \\\n",
       "0    San Diego  California       San Diego             330            60   \n",
       "1  Los Angeles  California     Los Angeles             470           300   \n",
       "2       Denver    Colorado          Denver             150          5410   \n",
       "3      Atlanta     Georgia  Fulton, DeKalb             140           890   \n",
       "4    Nashville   Tennessee        Davidson             500           600   \n",
       "\n",
       "  pop_year  pop_density_sqmi  pop_metro pop_rank            climate  \n",
       "0     2020              4260    3299000       17          Semi-Arid  \n",
       "1     2020              8300   13201000        2          Semi-Arid  \n",
       "2     2020              4670    2964000       19          Semi-Arid  \n",
       "3     2020              3690    6144000        8  Humid Subtropical  \n",
       "4     2020              1420    1990000       36  Humid Subtropical  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate empty list to add/compare each page's data to\n",
    "data_list = []\n",
    "climate_types = []\n",
    "i=0\n",
    "# Loop through each city's page\n",
    "for city in city_id:\n",
    "    \n",
    "    wiki_html = requests.get(url + city_id[city])\n",
    "    wiki_html = bs(wiki_html.text)\n",
    "    assert city in wiki_html.find('title').text\n",
    "    \n",
    "    # Save the relevant table and its rows\n",
    "    table_data = wiki_html.find('table', attrs={'class':'ib-settlement'})\n",
    "    table_rows = table_data.findAll('tr')\n",
    "    \n",
    "    data_dict = {}\n",
    "   \n",
    "    # Extract the city data (remove any state datarmation)\n",
    "    data_dict['city'] = table_data.find('div', attrs={'class':'fn org'}).text.split(',')[0]\n",
    "\n",
    "    # Extract the state, county, population and elevation data\n",
    "    for row in table_rows:\n",
    "        headers= row.findAll('th')\n",
    "        for header in headers:\n",
    "            if re.fullmatch('State', header.text, flags=re.IGNORECASE):\n",
    "                # Extract the state\n",
    "                data_dict['state'] = row.find('td').text\n",
    "            elif re.fullmatch('County|Counties|City and County', header.text, flags=re.IGNORECASE):\n",
    "                # Extract the county (removing unnecessary characters)\n",
    "                #data_dict['county'] = row.find('td').text.split('[')[0].split(' County')[0]\n",
    "                data_dict['county'] = re.sub('[\\[\\d\\]]| County', '', row.find('td').text)\n",
    "            elif header.text.find('Population') != -1:\n",
    "                # Extract the census year\n",
    "                data_dict['pop_year'] = row.find('a').text.replace(',', '')\n",
    "            elif header.text.find('Metro') != -1:\n",
    "                # Extract the metro population and US population rank\n",
    "                data_dict['pop_metro'] = round(int(row.find('td').text.split(' (')[0].replace(',', '')), -3)\n",
    "                data_dict['pop_rank'] = row.find('td').text.split(' (')[1][:-3].replace(',', '')\n",
    "            elif header.text.find('Density') != -1:\n",
    "                # Extract the population density (account for UNICODE and retain only standard measurement)\n",
    "                data_dict['pop_density_sqmi'] = int(round(float(row.find('td').text.replace('\\xa0', ' ').replace(',', '').split('/sq mi')[0]), -1))\n",
    "            elif header.text.find('Land') != -1:\n",
    "                # Extract the land area\n",
    "                data_dict['city_area_sqmi'] = int(round(float(row.find('td').text.replace('\\xa0', ' ').split(' sq')[0].replace(',', '')), -1))\n",
    "            elif header.text.find('Elevation') != -1:\n",
    "                # Extract the USGS elevation (account for UNICODE and retain only standard measurement)\n",
    "                elev_text = row.find('td').text.replace('\\xa0', ' ').split(' ft')[0].replace(',', '')\n",
    "                # When the elevation uses a range, use the average\n",
    "                if '–' in elev_text:\n",
    "                    data_dict['elevation_ft'] = int(round(np.mean([int(elev_text.split('–')[0]), int(elev_text.split('–')[1])]), -1))\n",
    "                elif ' to ' in elev_text:\n",
    "                    data_dict['elevation_ft'] = int(round(np.mean([int(elev_text.split(' to ')[0]), int(elev_text.split(' to ')[1])]), -1))\n",
    "                else:\n",
    "                    data_dict['elevation_ft'] = int(round(int(elev_text), -1))\n",
    "\n",
    "    # Extract the climate data, accounting for multiple variations per city\n",
    "    html_links = wiki_html.findAll('a')\n",
    "    city_climates = []\n",
    "    for link in html_links:\n",
    "        link_title = link.get('title')\n",
    "        if link_title:\n",
    "            match = re.fullmatch('.+\\sclimate', link.text, flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                city_climates.append(link.text[:-8].title())\n",
    "                \n",
    "    # Compare the city's climate type(s) to existing ones, giving precedence to common types\n",
    "    if len(city_climates) == 1:\n",
    "        # City has only one climate type, so just add it\n",
    "        data_dict['climate'] = city_climates[0].replace('Continental ', '')\n",
    "        climate_types.append(city_climates[0].replace('Continental ', ''))\n",
    "    elif len(city_climates) > 1:\n",
    "        # City has multiple (synonymous) climate types, so look for common existing types\n",
    "        common_climate = [climate for climate in city_climates if climate in climate_types]\n",
    "        if len(common_climate) == 0:\n",
    "            data_dict['climate'] = city_climates[0]\n",
    "            climate_types.append(city_climates[0])\n",
    "        else:\n",
    "            data_dict['climate'] = common_climate[0]   \n",
    "    \n",
    "    data_list.append(data_dict)\n",
    "    i+=1\n",
    "city_data = pd.DataFrame(data_list)\n",
    "city_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf364b3",
   "metadata": {},
   "source": [
    "### Export the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05535577",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data.to_excel(r'../data/city_data.xlsx', sheet_name='city_data', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
