{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5732fdd",
   "metadata": {},
   "source": [
    "## Webscraping for General City Information\n",
    "\n",
    "This notebook uses webscraping to collect general city data for a comparative study of air quality between nine cities spread throughout the US:\n",
    "\n",
    "1.   Seattle, Washington\n",
    "2.   San Diego, California\n",
    "3.   Phoenix, Arizona\n",
    "4.   Minneapolis, Minnesota\n",
    "5.   Denver, Colorado\n",
    "6.   Austin, Texas\n",
    "7.   Philadelphia, Pennsylvania\n",
    "8.   Nashville, Tennessee\n",
    "9.   Jacksonville, Florida\n",
    "\n",
    "The data gathered in this notebook is available at: https://en.wikipedia.org/wiki/. The resulting data is saved as a DataFrame and exported as a CSV file titled _'city_data.csv'_ and an Excel file titled _'city_data.xlsx'_.\n",
    "\n",
    "NOTES:<br>\n",
    "- The 'metro' population is used as a population gauge as it is the most consistently labeled. This value is defined as 'A region that consists of a densely populated urban agglomeration and its surrounding territories sharing industries, commercial areas, transport network, infrastructures and housing. Metropolitan areas typically include satellite cities, towns and intervening rural areas that are socioeconomically tied to the principal cities or urban core, often measured by commuting patterns (_source: https://en.wikipedia.org/wiki/Metropolitan_area_).<br>\n",
    "- The city's population rank is based on wikipedia's \"List of United States cities by population\" (_source: https://en.wikipedia.org//wiki/List_of_United_States_cities_by_population_)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0328f3",
   "metadata": {},
   "source": [
    "### Import the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31103a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b070c1c",
   "metadata": {},
   "source": [
    "### Declare the Static Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f8df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The url ids for each city\n",
    "city_id = {'Seattle':'Seattle',\n",
    "           'San Diego':'San_Diego',\n",
    "           'Phoenix':'Phoenix,_Arizona',\n",
    "           'Minneapolis':'Minneapolis',\n",
    "           'Denver':'Denver',\n",
    "           'Austin':'Austin',\n",
    "           'Philadelphia':'Philadelphia',\n",
    "           'Nashville':'Nashville',\n",
    "           'Jacksonville':'Jacksonville'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The common url\n",
    "url = 'https://en.wikipedia.org/wiki/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5354c24",
   "metadata": {},
   "source": [
    "### Loop through Each City's Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9572b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initiate empty lists to add/compare each page's data to\n",
    "data_list = []\n",
    "climate_types = []\n",
    "\n",
    "# Loop through each city's webpage\n",
    "for city in city_id:\n",
    "     \n",
    "    # Retrieve and format the html code\n",
    "    wiki_html = requests.get(url + city_id[city])\n",
    "    wiki_html = bs(wiki_html.text)\n",
    "    assert city in wiki_html.find('title').text\n",
    "    \n",
    "    # Save the relevant table and its rows\n",
    "    table_data = wiki_html.find('table', attrs={'class':'ib-settlement'})\n",
    "    table_rows = table_data.findAll('tr')\n",
    "   \n",
    "    # Extract the city data (remove any state datarmation)\n",
    "    data_dict = {}\n",
    "    data_dict['city'] = table_data.find('div', attrs={'class':'fn org'}).text.split(',')[0]\n",
    "\n",
    "    # Loop through each row and row header in the table, extracting state, county, population and elevation data\n",
    "    for row in table_rows:\n",
    "        \n",
    "        headers = row.findAll('th')\n",
    "        \n",
    "        for header in headers:\n",
    "            if re.fullmatch('State', header.text, flags=re.IGNORECASE):\n",
    "                # Extract the state\n",
    "                data_dict['state'] = row.find('td').text\n",
    "            elif re.fullmatch('County|Counties|City and County', header.text, flags=re.IGNORECASE):\n",
    "                # Extract the county (removing unnecessary characters)\n",
    "                #data_dict['county'] = row.find('td').text.split('[')[0].split(' County')[0]\n",
    "                data_dict['county'] = re.sub('[\\[\\d\\]]| County', '', row.find('td').text)\n",
    "            elif header.text.find('Population') != -1:\n",
    "                # Extract the census year\n",
    "                data_dict['pop_year'] = row.find('a').text\n",
    "            elif header.text.find('Metro') != -1:\n",
    "                # Extract the metro population and US population rank\n",
    "                if not(re.search('GMP', header.text)):\n",
    "                    spl = row.find('td').text.split(' (')\n",
    "                    if spl[0].replace('\\xa0', ' ').find('sq mi') == -1:\n",
    "                        data_dict['pop_metro'] = round(int(row.find('td').text.split(' (')[0].replace('\\xa0', ' ').replace(',', '')), -3)\n",
    "                        rank_spl = row.find('td').text.split(' (')[1].split(' ')\n",
    "                        if len(rank_spl) == 1:\n",
    "                            data_dict['pop_rank'] = int(rank_spl[0][:-3])\n",
    "                        else:\n",
    "                            data_dict['pop_rank'] = int(rank_spl[1][:-3])\n",
    "            elif header.text.find('Density') != -1:\n",
    "                # Extract the population density (account for UNICODE and retain only standard measurement)\n",
    "                data_dict['pop_density_sqmi'] = int(round(float(row.find('td').text.replace('\\xa0', ' ').replace(',', '').split('/sq mi')[0]), -1))\n",
    "            elif header.text.find('Land') != -1:\n",
    "                # Extract the land area\n",
    "                data_dict['city_area_sqmi'] = int(round(float(row.find('td').text.replace('\\xa0', ' ').split(' sq')[0].replace(',', '')), -1))\n",
    "            elif header.text.find('Elevation') != -1:\n",
    "                # Extract the USGS elevation (account for UNICODE and retain only standard measurement)\n",
    "                elev_text = row.find('td').text.replace('\\xa0', ' ').split(' ft')[0].replace(',', '')\n",
    "                # When the elevation uses a range, use the average\n",
    "                if '–' in elev_text:\n",
    "                    data_dict['elevation_ft'] = int(round(np.mean([int(elev_text.split('–')[0]), int(elev_text.split('–')[1])]), -1))\n",
    "                elif ' to ' in elev_text:\n",
    "                    data_dict['elevation_ft'] = int(round(np.mean([int(elev_text.split(' to ')[0]), int(elev_text.split(' to ')[1])]), -1))\n",
    "                else:\n",
    "                    data_dict['elevation_ft'] = int(round(int(elev_text), -1))\n",
    "                    \n",
    "            # End of row header loop\n",
    "            \n",
    "        # End of row loop\n",
    "        \n",
    "    # Extract the climate data, accounting for multiple variations per city\n",
    "    html_links = wiki_html.findAll('a')\n",
    "    city_climates = []\n",
    "    \n",
    "    for link in html_links:\n",
    "        link_title = link.get('title')\n",
    "        if link_title:\n",
    "            match = re.fullmatch('.+\\sclimate', link.text, flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                city_climates.append(link.text[:-8].title())\n",
    "                \n",
    "    # Compare the city's climate type(s) to existing ones, giving precedence to common types\n",
    "    if len(city_climates) == 1:\n",
    "        # City has only one climate type, so just add it\n",
    "        data_dict['climate'] = city_climates[0].replace('Continental ', '')\n",
    "        climate_types.append(city_climates[0].replace('Continental ', ''))\n",
    "    elif len(city_climates) > 1:\n",
    "        # City has multiple (synonymous) climate types, so look for common existing types\n",
    "        common_climate = [climate for climate in city_climates if climate in climate_types]\n",
    "        if len(common_climate) == 0:\n",
    "            data_dict['climate'] = city_climates[0]\n",
    "            climate_types.append(city_climates[0])\n",
    "        else:\n",
    "            data_dict['climate'] = common_climate[0]   \n",
    "    \n",
    "    data_list.append(data_dict)\n",
    "    \n",
    "    # End of city webpage loop\n",
    "    \n",
    "city_data = pd.DataFrame(data_list)\n",
    "city_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf364b3",
   "metadata": {},
   "source": [
    "### Export the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05535577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "city_data.to_csv(r'../data/city_data.csv', index=False)\n",
    "city_data.to_excel(r'../data/city_data.xlsx', sheet_name='city_data', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
